{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Match Outcome Predictor\n",
    "\n",
    "The following code will build a soccer match outcome prediction model using nural networks with team embeddings and match history embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Concatenate, Dropout, Flatten, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers, models, optimizers, callbacks\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('matches_expanded.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df['home_ball_possession'] = df['home_ball_possession'].str.replace('%', '', regex=True).astype(float)\n",
    "df['home_pass_accuracy'] = df['home_pass_accuracy'].str.replace('%', '', regex=True).astype(float)\n",
    "df['away_ball_possession'] = df['away_ball_possession'].str.replace('%', '', regex=True).astype(float)\n",
    "df['away_pass_accuracy'] = df['away_pass_accuracy'].str.replace('%', '', regex=True).astype(float)\n",
    "\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Team Names to Integer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_encoder = LabelEncoder()\n",
    "df['home_team_id'] = team_encoder.fit_transform(df['home_team'])\n",
    "df['away_team_id'] = team_encoder.transform(df['away_team'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Historical Performace Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5  # Number of past matches to consider\n",
    "\n",
    "def get_past_stats(df, team_col, stat_col, N):\n",
    "    \"\"\"\n",
    "    Computes rolling average for the last N matches per team.\n",
    "    If rolling value is NaN (not enough history), fill it with the current match's stat.\n",
    "    \"\"\"\n",
    "    rolling_avg = df.groupby(team_col)[stat_col].transform(lambda x: x.shift(1).rolling(N, min_periods=1).mean())\n",
    "    \n",
    "    # Fill NaNs with the current match stat as a fallback\n",
    "    rolling_avg = rolling_avg.fillna(df[stat_col])\n",
    "    \n",
    "    return rolling_avg\n",
    "\n",
    "historical_features = [\"goals\", \"ball_possession\", \"pass_accuracy\", \"total_shots\", \"expected_goals\"]\n",
    "\n",
    "for feature in historical_features:\n",
    "    home_feature_col = f'home_{feature}'\n",
    "    away_feature_col = f'away_{feature}'\n",
    "\n",
    "    df[f'{home_feature_col}_hist'] = get_past_stats(df, 'home_team', home_feature_col, N=5)\n",
    "    df[f'{away_feature_col}_hist'] = get_past_stats(df, 'away_team', away_feature_col, N=5)\n",
    "\n",
    "\n",
    "df.head(10)\n",
    "df.to_csv('cleaned_matches_expanded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [col for col in df.columns if '_hist' in col or col in [\n",
    "    'home_ball_possession', 'home_pass_accuracy', 'home_total_shots', 'home_expected_goals'\n",
    "    'away_ball_possession', 'away_pass_accuracy', 'away_total_shots', 'away_expected_goals'\n",
    "]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encode Target (Win/Draw/Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming perspective is home team\n",
    "# Win = 2, Draw = 0, Loss = 1\n",
    "\n",
    "outcome_encoder = LabelEncoder()\n",
    "df['outcome_encoded'] = outcome_encoder.fit_transform(df['match_outcome'])\n",
    "y = to_categorical(df['outcome_encoded'], num_classes=3)\n",
    "\n",
    "print(df.columns)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting Data into Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[numerical_cols]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(128, activation='relu')(input_layer)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and Evalutate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities on the test set\n",
    "y_test_probs = model.predict(X_test)\n",
    "\n",
    "# Predicted class labels (Win, Draw, Loss)\n",
    "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "\n",
    "# True class labels (converted back from one-hot encoding)\n",
    "y_test_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test_true, y_test_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_true, y_test_pred, target_names=outcome_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_labels = outcome_encoder.inverse_transform(y_test_pred)\n",
    "y_test_true_labels = outcome_encoder.inverse_transform(y_test_true)\n",
    "# Build a dataframe to compare actual vs predicted\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Actual Outcome': y_test_true_labels,\n",
    "    'Predicted Outcome': y_test_pred_labels,\n",
    "    'Confidence': np.max(y_test_probs, axis=1),\n",
    "    'Win Prob': y_test_probs[:, outcome_encoder.transform(['Win'])[0]],\n",
    "    'Draw Prob': y_test_probs[:, outcome_encoder.transform(['Draw'])[0]],\n",
    "    'Loss Prob': y_test_probs[:, outcome_encoder.transform(['Loss'])[0]]\n",
    "})\n",
    "\n",
    "# Add 'Correct' column (True if prediction matches actual)\n",
    "test_results_df['Correct'] = (test_results_df['Actual Outcome'] == test_results_df['Predicted Outcome'])\n",
    "\n",
    "# Show the first few predictions\n",
    "print(test_results_df.head())\n",
    "test_results_df.to_csv('test_results_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_true_labels, y_test_pred_labels, labels=outcome_encoder.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=outcome_encoder.classes_,\n",
    "            yticklabels=outcome_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Actual Outcome and calculate mean accuracy\n",
    "outcome_accuracy = test_results_df.groupby('Actual Outcome')['Correct'].mean()\n",
    "\n",
    "# Plot\n",
    "outcome_accuracy.plot(kind='bar', color='skyblue')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Prediction Accuracy per Outcome on Test Set')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define low confidence threshold\n",
    "low_confidence_threshold = 0.6\n",
    "\n",
    "# Create a column marking low-confidence wrong predictions\n",
    "test_results_df['Low-Confidence Wrong'] = (\n",
    "    (~test_results_df['Correct']) & (test_results_df['Confidence'] < low_confidence_threshold)\n",
    ")\n",
    "\n",
    "# Display only low-confidence wrong predictions\n",
    "low_conf_wrong_preds = test_results_df[test_results_df['Low-Confidence Wrong']]\n",
    "\n",
    "print(f\"Low-Confidence Wrong Predictions: {len(low_conf_wrong_preds)} matches\")\n",
    "print(low_conf_wrong_preds[['Actual Outcome', 'Predicted Outcome', 'Confidence']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of confidence scores\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(test_results_df['Confidence'], bins=20, color='mediumseagreen', edgecolor='black')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Number of Matches')\n",
    "plt.title('Distribution of Prediction Confidence (Test Set)')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "    \n",
    "    # Tune number of hidden layers (1â€“3)\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        # Tune units per layer\n",
    "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32)\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        \n",
    "        # Tune dropout rate\n",
    "        dropout_rate = hp.Float(f'dropout_rate_{i}', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(3, activation='softmax'))  # 3 classes (Win, Draw, Loss)\n",
    "    \n",
    "    # Tune learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,  # How many different models to try\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory='tuning_dir',\n",
    "    project_name='soccer_match_outcome'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Accuracy Across Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the tuning results\n",
    "tuner_results = tuner.oracle.get_best_trials(num_trials=20)\n",
    "\n",
    "# Extract val_accuracy from each trial\n",
    "val_accuracies = [trial.metrics.get_last_value('val_accuracy') for trial in tuner_results]\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, len(val_accuracies)+1), val_accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy across Tuning Trials')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no better model found using random search. We are going to use Bayesian optimization as well as smarter search ranges, better validation, and early stopping to hopefully find a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a new hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(X_train.shape[1] + 16,)))  # Adjust if you have team embeddings added\n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int(f'units_{i}', min_value=64, max_value=256, step=32),\n",
    "                activation='relu'\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Dropout(\n",
    "                rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=hp.Float('learning_rate', 1e-4, 5e-3, sampling='log')\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
